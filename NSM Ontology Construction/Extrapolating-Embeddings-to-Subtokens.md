Ambiguity: Medium
Relevance: Medium

- Work backwards from word embeddings to probabilistically determine subtoken embeddings for the original text
- Works from counterintuitive philosophical assumption that there exists a unit of linguistic meaning smaller than morphemes
- Work forwards using positional encoding methods to embed/vectorize multi-word/primitive token concept representations using semantic primitives

[Feed the embeddings into a multi-head attention module](Multi-Head-Attention-Mechanism)

[Previous](Embedding-Text-as-Ontology-Subgraph-Activations)
[Next](Embedding-Transition-Gradient.md)